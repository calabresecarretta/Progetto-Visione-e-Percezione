{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00f6d7f4-22e6-4706-b2ba-c795d11a27c6",
   "metadata": {},
   "source": [
    "1 - INSTALLAZIONE DELLE LIBRERIE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74bbbfe3-f523-454d-bb02-56bd1664f36a",
   "metadata": {},
   "source": [
    "!pip install -U keras\n",
    "!pip install -q tensorflow_datasets\n",
    "!pip install -U tensorflow\n",
    "!pip install tensorflow-examples\n",
    "!pip install numpy==1.26.0\n",
    "!pip install pycocotools\n",
    "!pip install scikit-image\n",
    "!pip install pyntcloud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ad3f26-9d9a-4a76-8505-77307f380c1f",
   "metadata": {},
   "source": [
    "2 - IMPORT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9e3122-4607-4295-82bf-8eff3c0cbb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow_examples.models.pix2pix import pix2pix\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "from pycocotools.coco import COCO\n",
    "import cv2  \n",
    "import os\n",
    "import ijson\n",
    "import json\n",
    "import urllib.request\n",
    "import zipfile\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import SeparableConv2D\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Conv2D, Conv2DTranspose, Concatenate, Input\n",
    "import torch\n",
    "from tensorflow.keras.models import load_model\n",
    "from skimage.filters import median\n",
    "from skimage.morphology import disk\n",
    "import plotly.graph_objs as go\n",
    "import tarfile\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import backend as K\n",
    "import keras\n",
    "from tkinter import Tk, filedialog"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330fb832-0b23-447b-bb9b-4a572f7bf761",
   "metadata": {},
   "source": [
    "3 - DIRECTORY DEL PROGETTO DA SOSTITUIRE CON LA PROPRIA CARTELLA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa6baa8-78b2-40a2-9edb-d8ad5b1500e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DIRECTORY DEL PROGETTO DA SOSTITUIRE \n",
    "project_dir = 'C:/Users/Tesisti/Desktop/CalabreseCarretta/Progetto'\n",
    "#VARIABILE PER ESEGUIRE L'ADDESTRAMENTO\n",
    "addestramento = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904d6f2c-a536-4b44-95e9-d636b102aeee",
   "metadata": {},
   "source": [
    "4 - DOWNLOAD DEL DATASET DI TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551e84da-351c-4e01-a1c2-4fe5755a30a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_extract(url, download_dir, extract_to=None):\n",
    "    if not os.path.exists(download_dir):\n",
    "        os.makedirs(download_dir)\n",
    "    filename = url.split(\"/\")[-1]\n",
    "    file_path = os.path.join(download_dir, filename)\n",
    "\n",
    "    # Scarica il file se non esiste già\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"Scaricando {filename}...\")\n",
    "        urllib.request.urlretrieve(url, file_path)\n",
    "        print(f\"Scaricato {filename} in {file_path}\")\n",
    "    else:\n",
    "        print(f\"{filename} esiste già, non è necessario scaricare.\")\n",
    "\n",
    "    # Estrai il file ZIP solo se necessario\n",
    "    if file_path.endswith(\".zip\") and extract_to:\n",
    "        # Controlla se la cartella di estrazione esiste e contiene file\n",
    "        if not os.path.exists(extract_to) or not os.listdir(extract_to):\n",
    "            with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
    "                print(f\"Estrazione di {filename} in {extract_to}...\")\n",
    "                zip_ref.extractall(extract_to)\n",
    "                print(f\"Estrazione completata.\")\n",
    "        else:\n",
    "            print(f\"I file in {extract_to} esistono già. Saltando l'estrazione.\")\n",
    "    return file_path\n",
    "\n",
    "# Percorso di salvataggio\n",
    "train_dataset_dir = os.path.join(project_dir, 'COCOdataset2017')\n",
    "images_dir = os.path.join(train_dataset_dir, 'images')\n",
    "annotations_dir = os.path.join(train_dataset_dir, 'annotations')\n",
    "\n",
    "# URL per scaricare i file\n",
    "urls = {\n",
    "    \"train_images\": \"http://images.cocodataset.org/zips/train2017.zip\",\n",
    "    \"val_images\": \"http://images.cocodataset.org/zips/val2017.zip\",\n",
    "    \"annotations\": \"http://images.cocodataset.org/annotations/annotations_trainval2017.zip\"\n",
    "}\n",
    "\n",
    "# Scarica le immagini di training\n",
    "train_images_dir = os.path.join(images_dir, 'train2017')\n",
    "download_and_extract(urls['train_images'], train_dataset_dir, images_dir)\n",
    "\n",
    "# Scarica le immagini di validazione\n",
    "val_images_dir = os.path.join(images_dir, 'val2017')\n",
    "download_and_extract(urls['val_images'], train_dataset_dir, images_dir)\n",
    "\n",
    "# Scarica le annotazioni\n",
    "download_and_extract(urls['annotations'], train_dataset_dir, train_dataset_dir)\n",
    "\n",
    "# Rinomina ed organizza le annotazioni\n",
    "annotations_zip_dir = os.path.join(train_dataset_dir, 'annotations')\n",
    "annotations_files = os.listdir(annotations_zip_dir)\n",
    "\n",
    "# Sposta i file .json nella cartella 'annotations'\n",
    "for file_name in annotations_files:\n",
    "    if file_name.endswith('.json'):\n",
    "        os.rename(\n",
    "            os.path.join(annotations_zip_dir, file_name),\n",
    "            os.path.join(annotations_dir, file_name)\n",
    "        )\n",
    "\n",
    "print(\"Scaricamento e organizzazione del dataset COCO 2017 completati!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798bce64-15a3-4fd5-b4a8-740afacf93f5",
   "metadata": {},
   "source": [
    "5 - DEFINIZIONE DELLE FUNZIONI PER CARICARE IMMAGINI E MASCHERE NEI DATASET TENSORFLOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97a0633-d7b5-4a0b-b474-0c2b67429949",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 128\n",
    "\n",
    "def get_image_id_from_path(image_path, coco):\n",
    "    \"\"\" Cerca l'ID dell'immagine nel dataset COCO basato sul nome del file \"\"\"\n",
    "    file_name = os.path.basename(image_path)\n",
    "    \n",
    "    # Ottieni tutte le immagini dal dataset COCO\n",
    "    img_info_list = coco.loadImgs(coco.getImgIds())\n",
    "    \n",
    "    # Cerca l'ID immagine corrispondente al nome del file\n",
    "    for img_info in img_info_list:\n",
    "        if img_info['file_name'] == file_name:\n",
    "            return img_info['id']\n",
    "    \n",
    "    # Se l'immagine non è stata trovata, solleva un'eccezione\n",
    "    raise ValueError(f\"ID immagine non trovato per il file: {file_name}\")\n",
    "\n",
    "def create_mask(image_id, coco):\n",
    "    annotation_ids = coco.getAnnIds(imgIds=image_id, iscrowd=False)\n",
    "    annotations = coco.loadAnns(annotation_ids)\n",
    "    #print(f\"Trovate {len(annotations)} annotazioni per l'immagine {image_id}\")\n",
    "    \n",
    "    mask = np.zeros((dim, dim), dtype=np.uint8)\n",
    "    for annotation in annotations:\n",
    "        annotation_mask = coco.annToMask(annotation)\n",
    "        resized_mask = cv2.resize(annotation_mask, (dim, dim), interpolation=cv2.INTER_NEAREST)\n",
    "        mask = np.maximum(mask, resized_mask)\n",
    "    return mask\n",
    "\n",
    "def load_image(image_path):\n",
    "    # Carica e decodifica l'immagine\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    \n",
    "    # Ridimensiona l'immagine a dimxdim\n",
    "    image = tf.image.resize(image, [dim, dim])\n",
    "    \n",
    "    # Normalizza l'immagine nel range [0, 1]\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    \n",
    "    return image\n",
    "\n",
    "def load_image_with_mask(image_path, coco_key):\n",
    "    \"\"\"Carica l'immagine e genera la maschera corrispondente.\"\"\"\n",
    "    # Converti il tensore in una stringa se necessario\n",
    "    image_path_str = image_path.numpy().decode('utf-8') if hasattr(image_path, 'numpy') else image_path\n",
    "    coco_key_str = coco_key.numpy().decode('utf-8') if hasattr(coco_key, 'numpy') else coco_key\n",
    "\n",
    "    # Controlla se il file esiste\n",
    "    if not os.path.exists(image_path_str):\n",
    "        print(f\"Immagine non trovata per il file: {image_path_str}\")\n",
    "        empty_image = tf.zeros((dim, dim, 3), dtype=tf.float32)\n",
    "        empty_mask = tf.zeros((dim, dim, 1), dtype=tf.float32)\n",
    "        return empty_image, empty_mask\n",
    "\n",
    "    try:\n",
    "        # Carica l'immagine\n",
    "        image = load_image(image_path_str)\n",
    "        image_id = get_image_id_from_path(image_path_str, coco_dict[coco_key_str])\n",
    "\n",
    "        # Crea la maschera associata\n",
    "        mask = create_mask(image_id, coco_dict[coco_key_str])\n",
    "\n",
    "        # Converti la maschera in un tensore di TensorFlow\n",
    "        mask = tf.convert_to_tensor(mask, dtype=tf.float32)\n",
    "        mask = tf.expand_dims(mask, axis=-1)  # Aggiungi una dimensione per renderla compatibile con (dim, dim, 1)\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Errore durante il caricamento di immagine o maschera: {e}\")\n",
    "        empty_image = tf.zeros((dim, dim, 3), dtype=tf.float32)\n",
    "        empty_mask = tf.zeros((dim, dim, 1), dtype=tf.float32)\n",
    "        return empty_image, empty_mask\n",
    "\n",
    "def load_image_wrapper(image_path_tensor, coco_key_tensor):\n",
    "    \"\"\"Wrapper per usare tf.py_function nella pipeline TensorFlow.\"\"\"\n",
    "    image, mask = tf.py_function(func=load_image_with_mask,\n",
    "                                 inp=[image_path_tensor, coco_key_tensor],\n",
    "                                 Tout=[tf.float32, tf.float32])\n",
    "    \n",
    "    # Definisci le dimensioni e le forme per TensorFlow\n",
    "    image.set_shape([dim, dim, 3])\n",
    "    mask.set_shape([dim, dim, 1])\n",
    "    return image, mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c16d45-bdef-4d12-b066-133e2e25d25f",
   "metadata": {},
   "source": [
    "6 - DEFINIZIONI FUNZIONI PER OTTENERE UNA PARTE DEL DATASET E PER PREPROCESSARE IL DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cc129f-3824-44f8-8abd-4eb15bdd36f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funzioni per ottenere un sottoinsieme del dataset e preprocessarlo\n",
    "def get_small_dataset(dataset, fraction = 0.5):\n",
    "    dataset_size = tf.data.experimental.cardinality(dataset).numpy()\n",
    "    small_dataset_size = int(dataset_size * fraction)\n",
    "    return dataset.take(small_dataset_size)\n",
    "\n",
    "def preprocess_dataset(dataset):\n",
    "    def preprocess(x, y=None):\n",
    "        x = tf.cast(x, tf.float32)\n",
    "        if y is not None:  # Se y è presente, preprocessiamo anche la maschera\n",
    "            y = tf.cast(y, tf.float32)\n",
    "            return x, y\n",
    "        else:\n",
    "            return x  # Se y non è presente, restituiamo solo x\n",
    "\n",
    "    return dataset.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd2b39c-30d5-48bb-aa7b-392a03632d9b",
   "metadata": {},
   "source": [
    "7 - CREAZIONI DATASET TENSORFLOW TRAIN E VAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b3e07c-c5c8-4119-b52b-13f8d806a444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caricare annotazioni COCO per il set di addestramento\n",
    "train_coco_annotation_path = os.path.join(train_dataset_dir, 'annotations', 'instances_train2017.json')\n",
    "train_coco = COCO(train_coco_annotation_path)\n",
    "\n",
    "# Caricare annotazioni COCO per il set di validazione\n",
    "val_coco_annotation_path = os.path.join(train_dataset_dir, 'annotations', 'instances_val2017.json')\n",
    "val_coco = COCO(val_coco_annotation_path)\n",
    "\n",
    "# Dizionario per memorizzare gli oggetti COCO\n",
    "coco_dict = {'train': train_coco, 'val': val_coco}\n",
    "\n",
    "# Directory delle immagini\n",
    "train_images_dir = os.path.join(train_dataset_dir, 'images', 'train2017')\n",
    "val_images_dir = os.path.join(train_dataset_dir, 'images', 'val2017')\n",
    "\n",
    "# Verifica i file nelle cartelle\n",
    "train_image_paths = [os.path.join(train_images_dir, fname) for fname in os.listdir(train_images_dir) if fname.endswith(('.jpg', '.jpeg', '.png'))]\n",
    "val_image_paths = [os.path.join(val_images_dir, fname) for fname in os.listdir(val_images_dir) if fname.endswith(('.jpg', '.jpeg', '.png'))]\n",
    "\n",
    "# Creazione dei dataset\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_image_paths, ['train'] * len(train_image_paths)))\n",
    "train_dataset = train_dataset.map(lambda x, y: load_image_wrapper(x, y), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((val_image_paths, ['val'] * len(val_image_paths)))\n",
    "val_dataset = val_dataset.map(lambda x, y: load_image_wrapper(x, y), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "# Applica batching e prefetching\n",
    "train_dataset = train_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "val_dataset = val_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# # Esegui l'addestramento su un sottoinsieme ridotto del dataset\n",
    "small_train_dataset = preprocess_dataset(get_small_dataset(train_dataset))\n",
    "small_val_dataset = preprocess_dataset(get_small_dataset(val_dataset))\n",
    "\n",
    "# Esegui l'addestramento su un sottoinsieme ridotto del dataset\n",
    "train_dataset = preprocess_dataset(train_dataset)\n",
    "val_dataset = preprocess_dataset(val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a34d750-c75b-4277-8129-b868eaa00eee",
   "metadata": {},
   "source": [
    "8 - DEFINIZIONE DEL MODELLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3472f097-ce5d-4097-a22f-e7fe8c1edc59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Blocco di attenzione\n",
    "def attention_block(x, g, inter_channel):\n",
    "    theta_x = layers.Conv2D(inter_channel, 1)(x)\n",
    "    phi_g = layers.Conv2D(inter_channel, 1)(g)\n",
    "    concat_xg = layers.add([theta_x, phi_g])\n",
    "    relu_xg = layers.Activation('relu')(concat_xg)\n",
    "    psi = layers.Conv2D(1, 1, activation='sigmoid')(relu_xg)\n",
    "    return layers.multiply([x, psi])\n",
    "\n",
    "def create_unet_model_with_vgg16(input_shape=(dim, dim, 3)):\n",
    "    # Carica il modello VGG16 preaddestrato senza il classificatore finale\n",
    "    vgg16 = VGG16(include_top=False, weights='imagenet', input_shape=input_shape)\n",
    "\n",
    "    #Blocca i pesi della VGG16 durante l'addestramento\n",
    "    for layer in vgg16.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    " \n",
    "    conv1 = vgg16.get_layer('block1_conv2')(vgg16.get_layer('block1_conv1')(inputs))\n",
    "    conv1 = layers.BatchNormalization()(conv1)\n",
    "    pool1 = vgg16.get_layer('block1_pool')(conv1)\n",
    "    \n",
    "    conv2 = vgg16.get_layer('block2_conv2')(vgg16.get_layer('block2_conv1')(pool1))\n",
    "    conv2 = layers.BatchNormalization()(conv2)\n",
    "    pool2 = vgg16.get_layer('block2_pool')(conv2)\n",
    "    \n",
    "    conv3 = vgg16.get_layer('block3_conv3')(vgg16.get_layer('block3_conv2')(vgg16.get_layer('block3_conv1')(pool2)))\n",
    "    conv3 = layers.BatchNormalization()(conv3)\n",
    "    pool3 = vgg16.get_layer('block3_pool')(conv3)\n",
    "    \n",
    "    conv4 = vgg16.get_layer('block4_conv3')(vgg16.get_layer('block4_conv2')(vgg16.get_layer('block4_conv1')(pool3)))\n",
    "    drop4 = layers.SpatialDropout2D(0.5)(conv4)\n",
    "    pool4 = vgg16.get_layer('block4_pool')(drop4)\n",
    "    \n",
    "    # Bottleneck con convoluzioni dilatate e regolarizzazione L2\n",
    "    conv5 = layers.Conv2D(1024, 3, activation='relu', padding='same', dilation_rate=2, kernel_regularizer=regularizers.l2(0.0005))(pool4)\n",
    "    conv5 = layers.BatchNormalization()(conv5)\n",
    "    conv5 = layers.Conv2D(1024, 3, activation='relu', padding='same', kernel_regularizer=regularizers.l2(0.0005))(conv5)\n",
    "    conv5 = layers.BatchNormalization()(conv5)\n",
    "    drop5 = layers.SpatialDropout2D(0.5)(conv5)\n",
    "    \n",
    "    # Decoder con blocchi di attenzione\n",
    "    up6 = layers.Conv2D(512, 2, activation='relu', padding='same')(layers.UpSampling2D(size=(2, 2))(drop5))\n",
    "    merge6 = layers.concatenate([drop4, up6], axis=3)\n",
    "    conv6 = layers.DepthwiseConv2D(3, padding='same')(merge6)\n",
    "    conv6 = layers.Conv2D(512, 1, activation='relu', kernel_regularizer=regularizers.l2(0.0005))(conv6)\n",
    "    conv6 = layers.BatchNormalization()(conv6)\n",
    "    conv6 = layers.Dropout(0.35)(conv6)\n",
    "    \n",
    "    # Blocco di attenzione per `conv6`\n",
    "    attention6 = attention_block(conv6, up6, 512)\n",
    "    \n",
    "    # Decoder livello successivo\n",
    "    up7 = layers.Conv2D(256, 2, activation='relu', padding='same')(layers.UpSampling2D(size=(2, 2))(attention6))\n",
    "    merge7 = layers.concatenate([conv3, up7], axis=3)\n",
    "    conv7 = layers.DepthwiseConv2D(3, padding='same')(merge7)\n",
    "    conv7 = layers.Conv2D(256, 1, activation='relu', kernel_regularizer=regularizers.l2(0.0005))(conv7)\n",
    "    conv7 = layers.BatchNormalization()(conv7)\n",
    "    conv7 = layers.Dropout(0.3)(conv7)\n",
    "    \n",
    "    # Blocco di attenzione per `conv7`\n",
    "    attention7 = attention_block(conv7, up7, 256)\n",
    "    \n",
    "    # Decoder livello successivo\n",
    "    up8 = layers.Conv2D(dim, 2, activation='relu', padding='same')(layers.UpSampling2D(size=(2, 2))(attention7))\n",
    "    merge8 = layers.concatenate([conv2, up8], axis=3)\n",
    "    conv8 = layers.DepthwiseConv2D(3, padding='same')(merge8)\n",
    "    conv8 = layers.Conv2D(dim, 1, activation='relu', kernel_regularizer=regularizers.l2(0.0005))(conv8)\n",
    "    conv8 = layers.BatchNormalization()(conv8)\n",
    "    conv8 = layers.Dropout(0.3)(conv8)\n",
    "    \n",
    "    # Blocco di attenzione per `conv8`\n",
    "    attention8 = attention_block(conv8, up8, dim)\n",
    "    \n",
    "    # Decoder livello finale\n",
    "    up9 = layers.Conv2D(64, 2, activation='relu', padding='same')(layers.UpSampling2D(size=(2, 2))(attention8))    \n",
    "    merge9 = layers.concatenate([conv1, up9], axis=3)\n",
    "    conv9 = layers.DepthwiseConv2D(3, padding='same')(merge9)\n",
    "    conv9 = layers.Conv2D(64, 1, activation='relu', kernel_regularizer=regularizers.l2(0.0005))(conv9)\n",
    "    conv9 = layers.BatchNormalization()(conv9)\n",
    "    conv9 = layers.Dropout(0.3)(conv9)\n",
    "    \n",
    "    # Blocco di attenzione per `conv9`\n",
    "    attention9 = attention_block(conv9, up9, 64)\n",
    "    \n",
    "    # Output layer\n",
    "    outputs = layers.Conv2D(1, 1, activation='sigmoid')(attention9)\n",
    "    \n",
    "    model = Model(inputs, outputs)\n",
    "\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4b741c-f0a6-4d92-a904-477f4556b44f",
   "metadata": {},
   "source": [
    "9 - DEFINIZIONE DELLE METRICHE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4b1637-52e9-43ab-9c5a-dfd4f011d4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funzione personalizzata per SSIM, IoU e F1 Score\n",
    "def ssim_metric(y_true, y_pred):\n",
    "    return tf.image.ssim(y_true, y_pred, max_val=1.0)\n",
    "\n",
    "def iou_metric(y_true, y_pred):\n",
    "    y_pred = tf.round(y_pred)  # Arrotondamento a 0 o 1 per segmentazione binaria\n",
    "    intersection = tf.reduce_sum(tf.cast(y_true * y_pred, tf.float32))\n",
    "    union = tf.reduce_sum(tf.cast(y_true + y_pred - y_true * y_pred, tf.float32))\n",
    "    iou = (intersection + 1e-10) / (union + 1e-10)  # Evita divisioni per zero\n",
    "    return iou\n",
    "\n",
    "def f1_score(y_true, y_pred):\n",
    "    y_pred = tf.round(y_pred)  # Arrotondamento a 0 o 1 per segmentazione binaria\n",
    "    intersection = tf.reduce_sum(y_true * y_pred)\n",
    "    precision = intersection / (tf.reduce_sum(y_pred) + 1e-10)\n",
    "    recall = intersection / (tf.reduce_sum(y_true) + 1e-10)\n",
    "    f1 = (2 * precision * recall) / (precision + recall + 1e-10)  # Evita divisioni per zero\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d75f6b-b03b-4d05-93c4-038cb3f17cee",
   "metadata": {},
   "source": [
    "10 - CREAZIONE, COMPILAZIONE E SALVATAGGIO DEL MODELLO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a5b119-3f40-47d8-aa52-9c6d293d75bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#percorso in cui salvare o da cui caricare il modello\n",
    "model_dir = os.path.join(project_dir, 'Modello')\n",
    "model_path = os.path.join(model_dir, 'modello.keras')\n",
    "\n",
    "@keras.saving.register_keras_serializable()\n",
    "def bce_dice_loss(y_true, y_pred):\n",
    "    bce_loss = tf.keras.losses.binary_crossentropy(y_true, y_pred)\n",
    "    dice_loss = 1 - (2 * tf.reduce_sum(y_true * y_pred) + 1) / (tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) + 1)\n",
    "    return bce_loss + dice_loss\n",
    "\n",
    "if addestramento:\n",
    "    # Creazione del modello\n",
    "    model = create_unet_model_with_vgg16()\n",
    "    \n",
    "    def dice_loss(y_true, y_pred):\n",
    "        y_true_f = K.flatten(y_true)\n",
    "        y_pred_f = K.flatten(y_pred)\n",
    "        intersection = K.sum(y_true_f * y_pred_f)\n",
    "        return 1 - (2. * intersection + 1) / (K.sum(y_true_f) + K.sum(y_pred_f) + 1)\n",
    "    \n",
    "    def bce_dice_loss(y_true, y_pred):\n",
    "        bce = K.binary_crossentropy(y_true, y_pred)\n",
    "        dice = dice_loss(y_true, y_pred)\n",
    "        return bce + dice\n",
    "        \n",
    "    # Compilazione del modello con le metriche personalizzate\n",
    "    model.compile(\n",
    "        optimizer = Adam(learning_rate=0.0001),\n",
    "        loss = bce_dice_loss,\n",
    "        metrics = ['accuracy']\n",
    "    )\n",
    "    \n",
    "    # Callback per Early Stopping e riduzione del learning rate\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', \n",
    "                                   patience=5, \n",
    "                                   restore_best_weights=True)\n",
    "    \n",
    "    reduce_lr = ReduceLROnPlateau(\n",
    "        monitor='val_loss',        \n",
    "        factor=0.5,               \n",
    "        patience = 2,              \n",
    "        min_lr=1e-6                \n",
    "    )\n",
    "    \n",
    "    # Addestramento del modello\n",
    "    history = model.fit(\n",
    "        #small_train_dataset,\n",
    "        train_dataset,\n",
    "        #validation_data = small_val_dataset,\n",
    "        validation_data = val_dataset,\n",
    "        epochs = 5,\n",
    "        callbacks = [early_stopping, reduce_lr]\n",
    "    )\n",
    "    \n",
    "    # Estrazione delle metriche dalla history\n",
    "    history_dict = history.history\n",
    "    train_loss = history_dict['loss']\n",
    "    val_loss = history_dict['val_loss']\n",
    "    \n",
    "    # Salvare le metriche in un file JSON\n",
    "    metriche_history = os.path.join(model_dir, 'metriche_history.json')\n",
    "    with open(metriche_history, 'w') as f:\n",
    "        json.dump({'train_loss': train_loss, 'val_loss': val_loss}, f)\n",
    "    \n",
    "    # Salva il modello addestrato in due formati\n",
    "    model.save(model_path)\n",
    "\n",
    "    # Grafico dell'andamento di train_loss e val_loss\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_loss, label='Training Loss')\n",
    "    plt.plot(val_loss, label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "else: \n",
    "    model = keras.models.load_model(model_path, custom_objects={'bce_dice_loss': bce_dice_loss})\n",
    "    #model = load_model(model_path)\n",
    "    model.trainable = False\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b5a558-7031-4f77-9d22-7a2e9eac5193",
   "metadata": {},
   "source": [
    "11 - VALUTAZIONE MODELLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96ef10b-80e0-44ce-9958-d957f7b3cdfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funzione per valutare le metriche medie e massime su un dataset e salvarle come immagine\n",
    "if addestramento:\n",
    "    def evaluate_metrics(model, dataset, save_path='metriche_tabella.png'):\n",
    "        # Inizializza liste per raccogliere i valori delle metriche per ogni batch\n",
    "        accuracy_values = []\n",
    "        iou_values = []\n",
    "        f1_values = []\n",
    "        ssim_values = []\n",
    "    \n",
    "        # Itera su ogni batch nel dataset di test\n",
    "        for x, y_true in dataset:\n",
    "            # Calcola le predizioni del modello\n",
    "            y_pred = model.predict(x, verbose=0)\n",
    "    \n",
    "            # Calcola le metriche per ogni batch\n",
    "            batch_accuracy = tf.keras.metrics.binary_accuracy(y_true, y_pred)\n",
    "            batch_iou = iou_metric(y_true, y_pred)\n",
    "            batch_f1 = f1_score(y_true, y_pred)\n",
    "            batch_ssim = ssim_metric(y_true, y_pred)\n",
    "    \n",
    "            # Aggiunge i valori delle metriche alla lista\n",
    "            accuracy_values.extend(batch_accuracy.numpy())\n",
    "            iou_values.append(batch_iou.numpy())\n",
    "            f1_values.append(batch_f1.numpy())\n",
    "            ssim_values.append(batch_ssim.numpy())\n",
    "    \n",
    "        # Calcola media e massimo per ciascuna metrica\n",
    "        metrics_summary = {\n",
    "            'accuracy': {\n",
    "                'mean': np.mean(accuracy_values),\n",
    "                'max': np.max(accuracy_values)\n",
    "            },\n",
    "            'iou': {\n",
    "                'mean': np.mean(iou_values),\n",
    "                'max': np.max(iou_values)\n",
    "            },\n",
    "            'f1_score': {\n",
    "                'mean': np.mean(f1_values),\n",
    "                'max': np.max(f1_values)\n",
    "            },\n",
    "            'ssim': {\n",
    "                'mean': np.mean(ssim_values),\n",
    "                'max': np.max(ssim_values)\n",
    "            }\n",
    "        }\n",
    "    \n",
    "        # Creazione di una tabella con matplotlib\n",
    "        fig, ax = plt.subplots(figsize=(8, 4))\n",
    "        ax.axis('tight')\n",
    "        ax.axis('off')\n",
    "        table_data = [\n",
    "            [\"Metric\", \"Mean\", \"Max\"],\n",
    "            [\"Accuracy\", f\"{metrics_summary['accuracy']['mean']:.4f}\", f\"{metrics_summary['accuracy']['max']:.4f}\"],\n",
    "            [\"IoU\", f\"{metrics_summary['iou']['mean']:.4f}\", f\"{metrics_summary['iou']['max']:.4f}\"],\n",
    "            [\"F1 Score\", f\"{metrics_summary['f1_score']['mean']:.4f}\", f\"{metrics_summary['f1_score']['max']:.4f}\"],\n",
    "            [\"SSIM\", f\"{metrics_summary['ssim']['mean']:.4f}\", f\"{metrics_summary['ssim']['max']:.4f}\"]\n",
    "        ]\n",
    "        table = ax.table(cellText=table_data, colLabels=None, cellLoc='center', loc='center')\n",
    "        table.auto_set_font_size(False)\n",
    "        table.set_fontsize(10)\n",
    "        table.scale(1.2, 1.2)  # Scaling della tabella per migliorare leggibilità\n",
    "    \n",
    "        # Salva l'immagine\n",
    "        plt.savefig(save_path, bbox_inches='tight')\n",
    "        plt.close(fig)\n",
    "    \n",
    "        return metrics_summary\n",
    "    \n",
    "    # Esempio di utilizzo con il dataset di test\n",
    "    metriche_tabella = os.path.join(model_dir, 'metriche_tabella.png')\n",
    "    metrics = evaluate_metrics(model, train_dataset, save_path=metriche_tabella)\n",
    "    #metrics = evaluate_metrics(model, small_train_dataset, save_path=metriche_tabella)\n",
    "    print(metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93077c79-62d1-427d-b714-20be496b5f81",
   "metadata": {},
   "source": [
    "12 - DEFINIZIONE FUNZIONI PER OTTENERE LA DEPTH MAP E LE POINT CLOUD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1ae703-530a-4d38-a941-1cbffd15b341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funzione per caricare il modello MiDaS v3\n",
    "def load_midas_model():\n",
    "    model_type = \"DPT_Large\"  # Usa DPT_Large per maggiore accuratezza\n",
    "    midas = torch.hub.load(\"intel-isl/MiDaS\", model_type)\n",
    "    midas.eval()\n",
    "    transform = torch.hub.load(\"intel-isl/MiDaS\", \"transforms\").dpt_transform\n",
    "    return midas, transform\n",
    "\n",
    "# Funzione per ottenere la mappa di profondità utilizzando MiDaS v3\n",
    "def get_depth_map(image, midas, transform):\n",
    "    img_np = (image.numpy() * 255).astype(np.uint8)\n",
    "\n",
    "    if img_np.shape[-1] == 3:\n",
    "        img_np = cv2.cvtColor(img_np, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    input_tensor = transform(img_np).unsqueeze(0)\n",
    "    if input_tensor.dim() == 5:\n",
    "        input_tensor = input_tensor.squeeze(1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        depth_map = midas(input_tensor).squeeze().cpu().numpy()\n",
    "\n",
    "    depth_map_normalized = (depth_map - depth_map.min()) / (depth_map.max() - depth_map.min()) * 255\n",
    "    depth_map_normalized = depth_map_normalized.astype(np.uint8)\n",
    "\n",
    "    depth_map_filtered = median(depth_map_normalized, disk(3))\n",
    "    depth_map_filtered = depth_map_filtered.astype(np.uint8)\n",
    "\n",
    "    return depth_map_filtered\n",
    "\n",
    "# Funzione per creare la nuvola di punti 3D usando la mappa di profondità\n",
    "def image_to_3d_point_cloud(depth_map, intrinsics):\n",
    "    height, width = depth_map.shape\n",
    "    y_indices, x_indices = np.indices((height, width))\n",
    "    z_values = depth_map[y_indices, x_indices]\n",
    "\n",
    "    f_x = intrinsics['f_x']\n",
    "    f_y = intrinsics['f_y']\n",
    "    c_x = intrinsics['c_x']\n",
    "    c_y = intrinsics['c_y']\n",
    "\n",
    "    X = (x_indices - c_x)\n",
    "    Y = (y_indices - c_y)\n",
    "    Z = z_values\n",
    "\n",
    "    point_cloud_3d = np.column_stack((X.flatten(), Y.flatten(), Z.flatten()))\n",
    "    return point_cloud_3d\n",
    "\n",
    "# Parametri intrinseci della fotocamera\n",
    "intrinsics = {\n",
    "    'f_x': dim,  # Lunghezza focale sull'asse X\n",
    "    'f_y': dim,  # Lunghezza focale sull'asse Y\n",
    "    'c_x': 64,   # Centro ottico sull'asse X\n",
    "    'c_y': 64    # Centro ottico sull'asse Y\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6b6b4e-bfec-4a78-b5df-8e1387def636",
   "metadata": {},
   "source": [
    "13 - DEFINIZIONE FUNZIONE PER VISUALIZZARE IMMAGINE, MASCHERA ORIGINALE, MASCHERA PREDETTA, DEPTH MAP, POINT CLOUD E POINT CLOUD SEGMENTATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddaf241-c273-44ec-a6bf-15960b1eef1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_and_save_predictions_with_depth(model, dataset, midas, transform, intrinsics, num_samples=3, max_points=100000, has_masks=True, output_dir='./output'):\n",
    "    os.makedirs(output_dir, exist_ok=True)  # Ensure output directory exists\n",
    "    dataset_iter = iter(dataset)\n",
    "    samples_shown = 0\n",
    "\n",
    "    print(\"Starting image processing and saving...\")\n",
    "\n",
    "    for batch in dataset_iter:\n",
    "        if samples_shown >= num_samples:\n",
    "            break\n",
    "\n",
    "        if has_masks:\n",
    "            images, masks = batch\n",
    "        else:\n",
    "            images = batch\n",
    "\n",
    "        for j in range(images.shape[0]):\n",
    "            if samples_shown >= num_samples:\n",
    "                break\n",
    "\n",
    "            image = images[j]\n",
    "            if has_masks:\n",
    "                mask = masks[j]\n",
    "\n",
    "            # Perform prediction\n",
    "            pred = model.predict(tf.expand_dims(image, axis=0))\n",
    "            binary_pred = (pred > 0.5).astype(np.uint8).squeeze()\n",
    "\n",
    "            # Get the depth map\n",
    "            depth_map = get_depth_map(image, midas, transform)\n",
    "\n",
    "            # Estrai la nuvola di punti 3D\n",
    "            point_cloud_3d = image_to_3d_point_cloud(depth_map, intrinsics)\n",
    "\n",
    "            # Visualize and save all in a single plot\n",
    "            try:\n",
    "                plt.figure(figsize=(20, 10))\n",
    "                \n",
    "                # Original Image\n",
    "                plt.subplot(1, 4, 1)\n",
    "                plt.imshow((image.numpy() * 255).astype(\"uint8\"))\n",
    "                plt.title(\"Original Image\")\n",
    "                plt.axis(\"off\")\n",
    "\n",
    "                # Original Mask, if present\n",
    "                if has_masks:\n",
    "                    plt.subplot(1, 4, 2)\n",
    "                    plt.imshow((image.numpy() * 255).astype(\"uint8\"))\n",
    "                    plt.imshow(mask.numpy().squeeze(), cmap='jet', alpha=0.5)\n",
    "                    plt.title(\"Original Mask\")\n",
    "                    plt.axis(\"off\")\n",
    "\n",
    "                # Predicted Mask\n",
    "                plt.subplot(1, 4, 3)\n",
    "                plt.imshow((image.numpy() * 255).astype(\"uint8\"))\n",
    "                plt.imshow(binary_pred, cmap='jet', alpha=0.5)\n",
    "                plt.title(\"Predicted Mask\")\n",
    "                plt.axis(\"off\")\n",
    "\n",
    "                # Depth Map\n",
    "                plt.subplot(1, 4, 4)\n",
    "                plt.imshow(depth_map, cmap='gray')\n",
    "                plt.title(\"Depth Map\")\n",
    "                plt.axis(\"off\")\n",
    "\n",
    "                # Save combined figure\n",
    "                combined_path = f\"{output_dir}/image_{samples_shown}_combined.png\"\n",
    "                plt.savefig(combined_path)\n",
    "                print(f\"Combined image saved: {combined_path}\")\n",
    "                plt.show()\n",
    "\n",
    "                 # Visualizza e salva la nuvola di punti 3D non segmentata\n",
    "                fig_original = go.Figure(data=[go.Scatter3d(\n",
    "                    x=point_cloud_3d[:, 0],\n",
    "                    y=point_cloud_3d[:, 1],\n",
    "                    z=point_cloud_3d[:, 2],\n",
    "                    mode='markers',\n",
    "                    marker=dict(\n",
    "                        size=2,\n",
    "                        color=point_cloud_3d[:, 2],\n",
    "                        colorscale='Jet',\n",
    "                        opacity=0.5\n",
    "                    )\n",
    "                )])\n",
    "                fig_original.update_layout(\n",
    "                    scene=dict(\n",
    "                        xaxis_title='X coordinates',\n",
    "                        yaxis_title='Y coordinates',\n",
    "                        zaxis_title='Depth (Z)'\n",
    "                    ),\n",
    "                    title=\"Interactive 3D Point Cloud (No Segmentation)\"\n",
    "                )\n",
    "                html_path = f\"{output_dir}/point_cloud_{samples_shown}_original.html\"\n",
    "                fig_original.write_html(html_path)\n",
    "                print(f\"Nuvola di punti originale salvata: {html_path}\")\n",
    "                fig_original.show()  # Mostra la nuvola non segmentata in Jupyter\n",
    "\n",
    "                # Ridimensiona la maschera predetta per corrispondere alla risoluzione della mappa di profondità\n",
    "                binary_pred_resized = cv2.resize(binary_pred, (depth_map.shape[1], depth_map.shape[0]), interpolation=cv2.INTER_NEAREST)\n",
    "                binary_pred_flat = binary_pred_resized.flatten()\n",
    "\n",
    "                # Colori: rosso per l'oggetto (maschera == 1), blu per il resto (maschera == 0)\n",
    "                colors = np.where(binary_pred_flat == 1, 'red', 'blue')\n",
    "\n",
    "                # Visualizza e salva la nuvola di punti 3D segmentata\n",
    "                fig_segmented = go.Figure(data=[go.Scatter3d(\n",
    "                    x=point_cloud_3d[:, 0],\n",
    "                    y=point_cloud_3d[:, 1],\n",
    "                    z=point_cloud_3d[:, 2],\n",
    "                    mode='markers',\n",
    "                    marker=dict(\n",
    "                        size=2,\n",
    "                        color=colors,\n",
    "                        opacity=0.8\n",
    "                    )\n",
    "                )])\n",
    "\n",
    "                fig_segmented.update_layout(\n",
    "                    scene=dict(\n",
    "                        xaxis_title='X coordinates',\n",
    "                        yaxis_title='Y coordinates',\n",
    "                        zaxis_title='Depth (Z)'\n",
    "                    ),\n",
    "                    title=\"Interactive 3D Point Cloud with Object Coloring\"\n",
    "                )\n",
    "\n",
    "                # Salva la nuvola segmentata come HTML\n",
    "                html_segmented_path = f\"{output_dir}/point_cloud_{samples_shown}_segmented.html\"\n",
    "                fig_segmented.write_html(html_segmented_path)\n",
    "                print(f\"Nuvola di punti segmentata salvata: {html_segmented_path}\")\n",
    "                fig_segmented.show()  # Mostra la nuvola segmentata in Jupyter\n",
    "\n",
    "                # Increment sample count\n",
    "                samples_shown += 1\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error during saving and visualization: {e}\")\n",
    "\n",
    "            if samples_shown >= num_samples:\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b103e5-826c-4da5-a34c-f6929481b21a",
   "metadata": {},
   "source": [
    "14 - VISUALIZZAZIONE RISULTATI TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb1a328-d79c-4f70-9b8c-5d2c6f94eed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carica il modello MiDaS v3\n",
    "midas, transform = load_midas_model()\n",
    "\n",
    "# Esegui la visualizzazione con mappa di profondità e nuvola 3D\n",
    "visualize_and_save_predictions_with_depth(model, train_dataset, midas, transform, intrinsics, num_samples=15, max_points=100000, has_masks=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be67285c-bdd9-4ce9-8024-f42d414996ee",
   "metadata": {},
   "source": [
    "15 - DOWNLOAD DATASET DI TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed2c06a-6f9e-4e2f-98a4-115612c157b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Definire il link di download e la directory di destinazione\n",
    "url = \"https://www.robots.ox.ac.uk/~vgg/data/pets/data/images.tar.gz\"\n",
    "annotations_url = \"https://www.robots.ox.ac.uk/~vgg/data/pets/data/annotations.tar.gz\"\n",
    "test_dataset_dir = os.path.join(project_dir, 'oxford_pet_data')\n",
    "download_dir = test_dataset_dir\n",
    "\n",
    "# Step 2: Creare la directory per i dati scaricati\n",
    "if not os.path.exists(download_dir):\n",
    "    os.makedirs(download_dir)\n",
    "\n",
    "# Step 3: Scaricare il dataset delle immagini\n",
    "images_path = os.path.join(download_dir, \"images.tar.gz\")\n",
    "if not os.path.exists(images_path):\n",
    "    print(\"Scaricamento del dataset Oxford-IIIT Pet (immagini)...\")\n",
    "    urllib.request.urlretrieve(url, images_path)\n",
    "    print(\"Download delle immagini completato.\")\n",
    "else:\n",
    "    print(\"Il dataset delle immagini è già stato scaricato.\")\n",
    "\n",
    "# Step 4: Scaricare il dataset delle annotazioni\n",
    "annotations_path = os.path.join(download_dir, \"annotations.tar.gz\")\n",
    "if not os.path.exists(annotations_path):\n",
    "    print(\"Scaricamento delle annotazioni del dataset Oxford-IIIT Pet...\")\n",
    "    urllib.request.urlretrieve(annotations_url, annotations_path)\n",
    "    print(\"Download delle annotazioni completato.\")\n",
    "else:\n",
    "    print(\"Le annotazioni del dataset sono già state scaricate.\")\n",
    "\n",
    "# Step 5: Estrarre il file delle immagini (tar.gz)\n",
    "extracted_images_dir = os.path.join(download_dir, \"images\")\n",
    "if not os.path.exists(extracted_images_dir):\n",
    "    print(\"Estrazione delle immagini...\")\n",
    "    with tarfile.open(images_path, \"r:gz\") as tar:\n",
    "        tar.extractall(path=download_dir)\n",
    "    print(\"Estrazione delle immagini completata.\")\n",
    "else:\n",
    "    print(\"Le immagini sono già state estratte.\")\n",
    "\n",
    "# Step 6: Estrarre il file delle annotazioni (tar.gz)\n",
    "extracted_annotations_dir = os.path.join(download_dir, \"annotations\")\n",
    "if not os.path.exists(extracted_annotations_dir):\n",
    "    print(\"Estrazione delle annotazioni...\")\n",
    "    with tarfile.open(annotations_path, \"r:gz\") as tar:\n",
    "        tar.extractall(path=download_dir)\n",
    "    print(\"Estrazione delle annotazioni completata.\")\n",
    "else:\n",
    "    print(\"Le annotazioni sono già state estratte.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133b3353-e6e3-4fa5-b94a-a8ab058ce112",
   "metadata": {},
   "source": [
    "16 - CREAZIONI DATASET TENSORFLOW DI TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8246935-62f8-4d31-94ae-c86ed9f30b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory delle immagini di test\n",
    "test_image_dir = os.path.join(test_dataset_dir, 'images')\n",
    "test_image_paths = [os.path.join(test_image_dir, fname) for fname in os.listdir(test_image_dir) if fname.endswith(('.jpg', '.jpeg', '.png'))]\n",
    "\n",
    "# Crea il dataset TensorFlow\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_image_paths))\n",
    "test_dataset = test_dataset.map(load_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "test_dataset = test_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "test_dataset = preprocess_dataset(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ccb0b3-232c-42ab-b8b0-420a1e2c1720",
   "metadata": {},
   "source": [
    "17 - VISUALIZZAZIONE RISULTATI DATASET DI TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888cc2b0-8578-411a-a7fd-e4d84a007629",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_and_save_predictions_with_depth(model, test_dataset, midas, transform, intrinsics, num_samples=30, max_points=100000, has_masks=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347a44d0-1962-4bbf-8ccd-e4cc504560f3",
   "metadata": {},
   "source": [
    "18 TEST CON IMMAGINE SCELTA DA PC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b7e4f3-c29a-4e07-afcb-dfbd3be9f46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funzione per selezionare l'immagine dal PC\n",
    "def select_image():\n",
    "    # Usa un file dialog per scegliere un'immagine\n",
    "    root = Tk()\n",
    "    root.withdraw()  # Nasconde la finestra principale di Tkinter\n",
    "    root.attributes(\"-topmost\", 1)  # Porta la finestra in primo piano\n",
    "    file_path = filedialog.askopenfilename(filetypes=[(\"Image files\", \"*.jpg;*.jpeg;*.png;*.bmp\")])\n",
    "    root.destroy()\n",
    "    \n",
    "    if not file_path:\n",
    "        print(\"Nessun file selezionato.\")\n",
    "        return None\n",
    "\n",
    "    # Carica l'immagine usando la funzione load_image\n",
    "    return load_image(file_path)\n",
    "\n",
    "# Carica l'immagine\n",
    "image_tensor = select_image()\n",
    "\n",
    "if image_tensor is not None:\n",
    "    # Aggiungi la dimensione del batch\n",
    "    image_tensor = tf.expand_dims(image_tensor, axis=0)\n",
    "\n",
    "if image_tensor is not None:\n",
    "    # Crea un dataset simulato con una singola immagine per compatibilità con la funzione\n",
    "    dataset = tf.data.Dataset.from_tensors(image_tensor)\n",
    "\n",
    "    # Chiama la funzione di visualizzazione con questo dataset a immagine singola\n",
    "    output_dir='./output'\n",
    "    visualize_and_save_predictions_with_depth(\n",
    "        model=model,\n",
    "        dataset=dataset,\n",
    "        midas=midas,\n",
    "        transform=transform,\n",
    "        intrinsics=intrinsics,\n",
    "        num_samples=1,  # Una singola immagine\n",
    "        max_points=100000, has_masks=False\n",
    "    )\n",
    "else:\n",
    "    print(\"Nessuna immagine è stata selezionata o caricata.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b757b1a7-deab-4562-8817-40805ae7c0ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
